{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Implementing a neural network from scratch"
      ],
      "metadata": {
        "id": "FNuA1veSRThR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "RCYZJ79QWF-U",
        "outputId": "d5ebbdd3-b2a4-416d-a170-ae1cde04de6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "done loading data\n",
            "0\n",
            "conv1 done\n",
            "pool1 done\n",
            "conv2 done\n",
            "(32, 64, 11, 11)\n",
            "pool2 done\n",
            "conv3 done\n",
            "fc1 done\n",
            "fc2 done\n",
            "soft done\n",
            "torch.Size([32, 10])\n",
            "tensor([8, 9, 1, 8, 1, 1, 5, 6, 1, 6, 7, 9, 7, 0, 0, 3, 9, 8, 1, 0, 4, 8, 5, 9,\n",
            "        9, 1, 7, 0, 1, 5, 7, 3])\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "grad.shape: torch.Size([32, 10])\n",
            "torch.Size([32, 10])\n",
            "(32, 64)\n",
            "grad_shape: (32, 64, 3, 3)\n",
            "d_bias shape: (64, 1)\n",
            "shape:  (1, 64)\n",
            "d_bias shape: (64, 1)\n",
            "(32, 64, 1, 1)\n",
            "(64, 64, 3, 3)\n",
            "dx: (32, 64, 5, 5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-261fb58250d1>\u001b[0m in \u001b[0;36m<cell line: 335>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-261fb58250d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, x_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-261fb58250d1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ReLU derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ReLU derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-261fb58250d1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, learning_rate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dx:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_weights\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,64,1,1) (64,64,3,3) "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Conv2D:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size).astype(np.float32) / (kernel_size * kernel_size)\n",
        "        self.bias = np.zeros((out_channels, 1), dtype=np.float32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, in_channels, in_height, in_width = x.shape\n",
        "        out_height = in_height - self.kernel_size + 1\n",
        "        out_width = in_width - self.kernel_size + 1\n",
        "        out = np.zeros((batch_size, self.out_channels, out_height, out_width), dtype=np.float32)\n",
        "\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                #receptive_field = x[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
        "                receptive_field = np.array(x[:, :, i:i+self.kernel_size, j:j+self.kernel_size])\n",
        "\n",
        "                #w = self.weights.reshape(receptive_field.shape)\n",
        "                #z = receptive_field * self.weights\n",
        "                c = np.einsum('ijkl,mjkl->mijkl', receptive_field, self.weights)\n",
        "                c = np.sum(c, axis=(0,2,3,4))\n",
        "                #print(z.shape)\n",
        "                #y = np.sum(z, axis=(1,2,3)) \n",
        "                #print(\"y.shape:\",y.shape)\n",
        "                #out[:, :, i, j] = np.sum(receptive_field * self.weights, axis=(1,2,3)) + self.bias\n",
        "                c = c+self.bias\n",
        "                out[:, :, i, j] = c.transpose(1,0)\n",
        "\n",
        "                #break\n",
        "                \n",
        "        return out\n",
        "\n",
        "    def backward(self, grad, learning_rate):\n",
        "        batch_size, in_channels, in_height, in_width = self.x.shape\n",
        "        out_height = in_height - self.kernel_size + 1\n",
        "        out_width = in_width - self.kernel_size + 1\n",
        "\n",
        "        d_weights = np.zeros_like(self.weights, dtype=np.float32)\n",
        "        d_bias = np.zeros_like(self.bias, dtype=np.float32)\n",
        "        dx = np.zeros_like(self.x, dtype=np.float32)\n",
        "\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                receptive_field = self.x[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
        "                d_weights += np.sum(receptive_field * (grad[:, :, i, j])[:, :, np.newaxis, np.newaxis], axis=0)\n",
        "                grad = np.array(grad)\n",
        "                # print(\"grad_shape:\", grad.shape)\n",
        "                # print(\"d_bias shape:\", d_bias.shape)\n",
        "                # print(\"shape: \", np.sum(grad[:, :, i, j], axis=0, keepdims=True).shape)\n",
        "                d_bias += (np.sum(grad[:, :, i, j], axis=0, keepdims=True)).transpose(1,0)\n",
        "                # print(\"d_bias shape:\", d_bias.shape)\n",
        "                \n",
        "                # print((grad[:, :, i, j])[:, :, np.newaxis, np.newaxis].shape)\n",
        "                # print(self.weights.shape)\n",
        "                # print(\"dx:\", dx.shape)\n",
        "                dx[:, :, i:i+self.kernel_size, j:j+self.kernel_size] += np.sum((grad[:, :, i, j])[:, :, np.newaxis, np.newaxis] * self.weights, axis=1)\n",
        "\n",
        "        self.weights -= learning_rate * d_weights / batch_size\n",
        "        self.bias -= learning_rate * d_bias / batch_size\n",
        "\n",
        "        return dx\n",
        "    \n",
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, in_channels, in_height, in_width = x.shape\n",
        "        out_height = in_height // self.kernel_size\n",
        "        out_width = in_width // self.kernel_size\n",
        "        out = np.zeros((batch_size, in_channels, out_height, out_width), dtype=np.float32)\n",
        "\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                receptive_field = x[:, :, i*self.kernel_size:(i+1)*self.kernel_size, j*self.kernel_size:(j+1)*self.kernel_size]\n",
        "\n",
        "                out[:, :, i, j] = np.amax(receptive_field, axis=(2,3))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad):\n",
        "        batch_size, in_channels, in_height, in_width = grad.shape\n",
        "        out_height = in_height * self.kernel_size\n",
        "        out_width = in_width * self.kernel_size\n",
        "        dx = np.zeros((batch_size, in_channels, out_height, out_width), dtype=np.float32)\n",
        "\n",
        "        for i in range(in_height):\n",
        "            for j in range(in_width):\n",
        "                start_i = i * self.kernel_size\n",
        "                start_j = j * self.kernel_size\n",
        "                end_i = start_i + self.kernel_size\n",
        "                end_j = start_j + self.kernel_size\n",
        "                receptive_field = dx[:, :, start_i:end_i, start_j:end_j]\n",
        "                mask = (receptive_field == np.max(receptive_field, axis=(2,3))[:, :, None, None])\n",
        "                dx[:, :, start_i:end_i, start_j:end_j] += mask * (grad[:, :, i, j])[:, :, None, None]\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.weights = np.random.randn(out_features, in_features).astype(np.float32) * np.sqrt(2 / in_features)\n",
        "        self.bias = np.zeros((out_features, 1), dtype=np.float32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        out = np.dot(np.array(x).reshape(batch_size, -1), self.weights.T) + self.bias.T\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad, learning_rate):\n",
        "        batch_size = self.x.shape[0]\n",
        "        dx = np.dot(grad, self.weights)\n",
        "        dx = dx.reshape(batch_size, *self.x.shape[1:])\n",
        "        d_weights = np.dot(grad.T, self.x.reshape(batch_size, -1))\n",
        "        #c = np.sum(grad, axis=(1))\n",
        "        print(grad.shape)\n",
        "        grad = np.array(grad)\n",
        "        d_bias = np.sum(grad, axis=0, keepdims=True).T\n",
        "        self.weights -= learning_rate * d_weights / batch_size\n",
        "        self.bias -= learning_rate * d_bias / batch_size\n",
        "        return dx\n",
        "        # d_weights = np.dot(grad.T, self.x) / self.x.shape[0]\n",
        "        # d_bias = np.sum(grad, axis=0) / self.x.shape[0]\n",
        "        # dx = np.dot(grad, self.weights)\n",
        "        # self.weights -= learning_rate * d_weights\n",
        "        # self.bias -= learning_rate * d_bias.reshape(-1, 1)\n",
        "        # return dx\n",
        "    \n",
        "import torch.nn as nn\n",
        "\n",
        "class Net:\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        self.conv1 = Conv2D(3, 32, kernel_size=3)\n",
        "        self.pool1 = MaxPool2D(kernel_size=2)\n",
        "        self.conv2 = Conv2D(32, 64, kernel_size=5)\n",
        "        self.pool2 = MaxPool2D(kernel_size=2)\n",
        "        self.conv3 = Conv2D(64, 64, kernel_size=3)\n",
        "        self.fc1 = Linear(64 * 3 * 3, 64)\n",
        "        self.fc2 = Linear(64, 10)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = [self.conv1, self.pool1, self.conv2, self.pool2, self.conv3, self.fc1, self.fc2]\n",
        "\n",
        "        # Adam optimizer parameters\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-8\n",
        "        self.t = 0\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'weights'):\n",
        "                self.m[layer] = np.zeros_like(layer.weights)\n",
        "                self.v[layer] = np.zeros_like(layer.weights)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1.forward(x)\n",
        "\n",
        "        x = np.maximum(x, 0)  # ReLU activation function\n",
        "        print(\"conv1 done\")\n",
        "        x = self.pool1.forward(x)\n",
        "        print(\"pool1 done\")\n",
        "        x = self.conv2.forward(x)\n",
        "        \n",
        "        x = np.maximum(x, 0)  # ReLU activation function\n",
        "        print(\"conv2 done\")\n",
        "        print(x.shape)\n",
        "        x = self.pool2.forward(x)\n",
        "        print(\"pool2 done\") \n",
        "        x = self.conv3.forward(x)\n",
        "        x = np.maximum(x, 0)\n",
        "        print(\"conv3 done\")\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        #print(x.shape)\n",
        "        #x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = np.maximum(x, 0)  # ReLU activation function\n",
        "        print(\"fc1 done\")\n",
        "        x = self.fc2.forward(x)\n",
        "        print(\"fc2 done\")\n",
        "        x = torch.from_numpy(x)\n",
        "        x = self.softmax(x)\n",
        "        print(\"soft done\")\n",
        "        return x  \n",
        "    \n",
        "    \"\"\"def backward(self, grad):\n",
        "        grad = self.fc2.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.fc1.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = grad.reshape(grad.shape[0], self.conv2.out_channels, 5, 5)\n",
        "        grad = self.pool2.backward(grad)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative \n",
        "        grad = self.conv2.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.pool1.backward(grad)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.conv1.backward(grad, self.learning_rate)\n",
        "        return grad\"\"\"\n",
        "\n",
        "    def backward(self, grad):\n",
        "        grad = self.softmax(grad)\n",
        "        grad = self.fc2.backward(grad, self.learning_rate)\n",
        "        grad = self.fc1.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = grad.reshape(grad.shape[0], self.conv3.out_channels, 3, 3)\n",
        "        grad = self.conv3.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative \n",
        "        grad = self.pool2.backward(grad)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.conv2.backward(grad, self.learning_rate)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.pool1.backward(grad)\n",
        "        grad = np.maximum(grad, 0)  # ReLU derivative\n",
        "        grad = self.conv1.backward(grad, self.learning_rate)\n",
        "        return grad\n",
        "\n",
        "    \n",
        "    def update_weights(self):\n",
        "        self.t += 1\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'weights'):\n",
        "                self.m[layer] = self.beta1 * self.m[layer] + (1 - self.beta1) * layer.d_weights\n",
        "                self.v[layer] = self.beta2 * self.v[layer] + (1 - self.beta2) * layer.d_weights**2\n",
        "                m_hat = self.m[layer] / (1 - self.beta1**self.t)\n",
        "                v_hat = self.v[layer] / (1 - self.beta2**self.t)\n",
        "                layer.weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "                layer.bias -= self.learning_rate * layer.d_bias\n",
        "    \n",
        "    def train(self, x_train, y_train, x_val=None, y_val=None, epochs=10, batch_size=32):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_accs = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            print(epoch)\n",
        "            # shuffle the training data\n",
        "            # permutation = np.random.permutation(x_train.shape[0])\n",
        "            # x_train = x_train[permutation]\n",
        "            # y_train = y_train[permutation]\n",
        "            \n",
        "            loss = 0.0\n",
        "            # split the data into batches\n",
        "            num_batches = x_train.shape[0] // batch_size\n",
        "            for i in range(num_batches):\n",
        "                # get the current batch\n",
        "                start = i * batch_size\n",
        "                end = (i+1) * batch_size\n",
        "                x_batch = x_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "                \n",
        "                # forward pass\n",
        "                \n",
        "                y_pred = self.forward(x_batch)\n",
        "                print(y_pred.shape)\n",
        "                print(y_batch)\n",
        "                # compute loss and gradients\n",
        "                print(type(y_pred))\n",
        "                print(type(y_batch))\n",
        "                #loss += -np.mean(np.log(y_pred[np.arange(len(y_batch)), y_batch]))\n",
        "                #loss += -np.mean(np.log(y_pred[np.arange(len(y_batch)), y_batch]), dtype=np.float64)\n",
        "\n",
        "                loss = self.loss_fn(y_pred, y_batch)  \n",
        "\n",
        "                grad = self.softmax(y_pred)\n",
        "                grad[np.arange(len(y_batch)), y_batch] -= 1\n",
        "                grad /= len(y_batch)\n",
        "                print(type(grad))\n",
        "                print(\"grad.shape:\", grad.shape)\n",
        "                \n",
        "                self.backward(grad)\n",
        "                \n",
        "                # update weights\n",
        "                self.update_weights()\n",
        "                \n",
        "            # evaluate on training set\n",
        "            # train_loss = self.evaluate(x_train, y_train)\n",
        "            # train_losses.append(train_loss)\n",
        "            train_losses.append(loss / len(x_train))\n",
        "            \n",
        "            # evaluate on validation set, if provided\n",
        "            \"\"\"if x_val is not None and y_val is not None:\n",
        "                val_loss, val_acc = self.evaluate(x_val, y_val, accuracy=True)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}\")\n",
        "        \n",
        "        if x_val is not None and y_val is not None:\n",
        "            return train_losses, val_losses, val_accs\n",
        "        else:\n",
        "            return train_losses\"\"\"\n",
        "        \n",
        "        \n",
        "    \"\"\"def train(self, train_data_loader, val_data_loader, epochs=10):\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for images, labels in train_data_loader:\n",
        "                y_pred = self.forward(images)\"\"\"\n",
        "                \n",
        "    \n",
        "    def predict(self, x):\n",
        "        y_pred = self.forward(x)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "    \n",
        "    def evaluate(self, x, y, accuracy=False):\n",
        "        y_pred = self.forward(x)\n",
        "        loss, grad = self.loss(y_pred, y)\n",
        "        if accuracy:\n",
        "            acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
        "            return loss, acc\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # For cifar-10 you get pre-built loaders\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                              shuffle=True, num_workers=4)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "    \n",
        "    # Get x_train and y_train\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for batch in trainloader:\n",
        "        images, labels = batch\n",
        "        x_train.append(images)\n",
        "        y_train.append(labels)\n",
        "    x_train = torch.cat(x_train, dim=0)\n",
        "    y_train = torch.cat(y_train, dim=0)\n",
        "    \n",
        "    # Get x_test and y_test\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for batch in testloader:\n",
        "        images, labels = batch\n",
        "        x_test.append(images)\n",
        "        y_test.append(labels)\n",
        "    x_test = torch.cat(x_test, dim=0)\n",
        "    y_test = torch.cat(y_test, dim=0)\n",
        "    \n",
        "    print(\"done loading data\")\n",
        "\n",
        "    x = Net()\n",
        "    x.train(x_train, y_train)\n",
        "\n",
        "\n",
        " \n"
      ]
    }
  ]
}